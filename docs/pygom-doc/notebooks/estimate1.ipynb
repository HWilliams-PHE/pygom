{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Parameter Estimation 1\n",
    "\n",
    "## Estimation under square loss\n",
    "\n",
    "To ease the estimation process when given data, a separate module\n",
    "`ode_loss` has been constructed for observations coming from a single\n",
    "state. We demonstrate how to do it via two examples, first, a standard\n",
    "SIR model, then the Legrand SEIHFR model from [\\[Legrand2007\\]]() used\n",
    "for Ebola in `estimate2`.\n",
    "\n",
    "### SIR Model\n",
    "\n",
    "We set up an SIR model as seen previously in `sir`.\n",
    "\n",
    "In \\[176\\]: from pygom import SquareLoss, common_models\n",
    "\n",
    "In \\[179\\]: import numpy\n",
    "\n",
    "In \\[180\\]: import scipy.integrate\n",
    "\n",
    "In \\[184\\]: import matplotlib.pyplot\n",
    "\n",
    "In \\[185\\]: \\# Again, standard SIR model with 2 parameter. See the first\n",
    "script!\n",
    "\n",
    "In \\[191\\]: \\# define the parameters\n",
    "\n",
    "In \\[192\\]: paramEval = \\[('beta',0.5), ('gamma',1.0/3.0)\\]\n",
    "\n",
    "In \\[189\\]: \\# initialize the model\n",
    "\n",
    "In \\[190\\]: ode = common_models.SIR(paramEval)\n",
    "\n",
    "and we assume that we have perfect information about the $R$\n",
    "compartment.\n",
    "\n",
    "In \\[196\\]: x0 = \\[1, 1.27e-6, 0\\]\n",
    "\n",
    "In \\[197\\]: \\# Time, including the initial time t0 at t=0\n",
    "\n",
    "In \\[198\\]: t = numpy.linspace(0, 150, 1000)\n",
    "\n",
    "In \\[200\\]: \\# Standard. Find the solution.\n",
    "\n",
    "In \\[201\\]: solution = scipy.integrate.odeint(ode.ode, x0, t)\n",
    "\n",
    "In \\[202\\]: y = solution\\[:,1:3\\].copy()\n",
    "\n",
    "Initialize the class with some initial guess\n",
    "\n",
    "In \\[209\\]: \\# our initial guess\n",
    "\n",
    "In \\[210\\]: theta = \\[0.2, 0.2\\]\n",
    "\n",
    "In \\[176\\]: objSIR = SquareLoss(theta, ode, x0, t\\[0\\], t\\[1::\\],\n",
    "y\\[1::,:\\], \\['I','R'\\])\n",
    "\n",
    "Note that we need to provide the initial values, $x_{0}$ and $t_{0}$\n",
    "differently to the observations $y$ and the corresponding time $t$.\n",
    "Additionally, the state which the observation lies needs to be\n",
    "specified. Either a single state, or multiple states are allowed, as\n",
    "seen above.\n",
    "\n",
    "### Difference in gradient\n",
    "\n",
    "We have provided two different ways of obtaining the gradient, these are\n",
    "explained in `gradient` in a bit more detail. First, lets see how\n",
    "similar the output of the two methods are\n",
    "\n",
    "In \\[22\\]: objSIR.sensitivity()\n",
    "\n",
    "In \\[25\\]: objSIR.adjoint()\n",
    "\n",
    "and the time required to obtain the gradient for the SIR model under\n",
    "$\\theta = (0.2,0.2)$, previously entered.\n",
    "\n",
    "In \\[22\\]: %timeit objSIR.sensitivity()\n",
    "\n",
    "In \\[25\\]: %timeit objSIR.adjoint()\n",
    "\n",
    "Obviously, the amount of time taken for both method is dependent on the\n",
    "number of observations as well as the number of states. The effect on\n",
    "the adjoint method as the number of observations differs can be quite\n",
    "evident. This is because the adjoint method is under a discretization\n",
    "which loops in Python where as the forward sensitivity equations are\n",
    "solved simply via an integration. As the number of observation gets\n",
    "larger, the affect of the Python loop becomes more obvious.\n",
    "\n",
    "Difference in gradient is larger when there are less observations. This\n",
    "is because the adjoint method use interpolations on the output of the\n",
    "ode between each consecutive time points. Given solution over the same\n",
    "length of time, fewer discretization naturally leads to a less accurate\n",
    "interpolation. Note that the interpolation is currently performed using\n",
    "univaraite spline, due to the limitation of python packages. Ideally,\n",
    "one would prefer to use an (adaptive) Hermite or Chebyshev\n",
    "interpolation. Note how we ran the two gradient functions once before\n",
    "timing it, that is because we only find the properties (Jacobian,\n",
    "gradient) of the ode during runtime.\n",
    "\n",
    "### Optimized result\n",
    "\n",
    "Then standard optimization procedures with some suitable initial guess\n",
    "should yield the correct result. It is important to set the boundaries\n",
    "for compartmental models as we know that all the parameters are strictly\n",
    "positive. We put a less restrictive inequality here for demonstration\n",
    "purpose.\n",
    "\n",
    "In \\[211\\]: \\# what we think the bounds are\n",
    "\n",
    "In \\[212\\]: boxBounds = \\[(0.0,2.0),(0.0,2.0)\\]\n",
    "\n",
    "Then using the optimization routines in `scipy.optimize`, for example,\n",
    "the *SLSQP* method with the gradient obtained by forward sensitivity.\n",
    "\n",
    "In \\[208\\]: from scipy.optimize import minimize\n",
    "\n",
    "In \\[213\\]: res = minimize(fun=objSIR.cost,  \n",
    ".….: jac=objSIR.sensitivity, .….: x0=theta, .….: bounds=boxBounds, .….:\n",
    "method='SLSQP')\n",
    "\n",
    "In \\[214\\]: print(res)\n",
    "\n",
    "Other methods available in `scipy.optimize.minimize` can also be used,\n",
    "such as the *L-BFGS-B* and *TNC*. We can also use methods that accepts\n",
    "the exact Hessian such as *trust-ncg* but that should not be necessary\n",
    "most of the time."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
