{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Interval of Estimated Parameters\n",
    "\n",
    "After obtaining the *best* fit, it is natural to report both the point\n",
    "estimate and the confidence level at the $\\alpha$ level. The easiest way\n",
    "to do this is by invoking the normality argument and use Fisher\n",
    "information of the likelihood. As explained previously at the bottom of\n",
    "`gradient`, we can find the Hessian, $\\mathbf{H}$, or the approximated\n",
    "Hessian for the estimated parameters. The Cramer--Rao inequality, we\n",
    "know that\n",
    "\n",
    "$$Var(\\hat{\\theta}) \\ge \\frac{1}{I(\\theta)},$$\n",
    "\n",
    "where $I(\\theta)$ is the Fisher information, which is the Hessian\n",
    "subject to regularity condition. Given the Hessian, computing the\n",
    "confidence intervals is trivial. Note that this is also known as the\n",
    "asymptotic confidence interval where the normality comes from invoking\n",
    "the CLT. There are other ways of obtaining a confidence intervals, we\n",
    "will the ones implemented in the package. First, we will set up a SIR\n",
    "model as seen in `sir` which will be used throughout this page.\n",
    "\n",
    "In \\[1\\]: from pygom import NormalLoss, common_models\n",
    "\n",
    "In \\[2\\]: from pygom.utilR import qchisq\n",
    "\n",
    "In \\[3\\]: import numpy\n",
    "\n",
    "In \\[4\\]: import scipy.integrate\n",
    "\n",
    "In \\[5\\]: import matplotlib.pyplot as plt\n",
    "\n",
    "In \\[6\\]: import copy\n",
    "\n",
    "In \\[7\\]: ode = common_models.SIR(\\[('beta', 0.5), ('gamma', 1.0/3.0)\\])\n",
    "\n",
    "and we assume that we only have observed realization from the $R$\n",
    "compartment\n",
    "\n",
    "In \\[1\\]: x0 = \\[1, 1.27e-6, 0\\]\n",
    "\n",
    "In \\[2\\]: t = numpy.linspace(0, 150, 100).astype('float64')\n",
    "\n",
    "In \\[3\\]: ode.initial_values = (x0, t\\[0\\])\n",
    "\n",
    "In \\[4\\]: solution = ode.integrate(t\\[1::\\])\n",
    "\n",
    "In \\[5\\]: theta = \\[0.2, 0.2\\]\n",
    "\n",
    "In \\[6\\]: targetState = \\['R'\\]\n",
    "\n",
    "In \\[7\\]: targetStateIndex =\n",
    "numpy.array(ode.get_state_index(targetState))\n",
    "\n",
    "In \\[8\\]: y = solution\\[1::,targetStateIndex\\] + numpy.random.normal(0,\n",
    "0.01, (len(solution\\[1::,targetStateIndex\\]), 1))\n",
    "\n",
    "In \\[9\\]: yObv = y.copy()\n",
    "\n",
    "In \\[10\\]: objSIR = NormalLoss(theta, ode, x0, t\\[0\\], t\\[1::\\], y,\n",
    "targetState)\n",
    "\n",
    "In \\[11\\]: boxBounds = \\[(1e-8, 2.0), (1e-8, 2.0)\\]\n",
    "\n",
    "In \\[12\\]: boxBoundsArray = numpy.array(boxBounds)\n",
    "\n",
    "In \\[13\\]: xhat = objSIR.fit(theta, lb=boxBoundsArray\\[:,0\\],\n",
    "ub=boxBoundsArray\\[:,1\\])\n",
    "\n",
    "## Asymptotic\n",
    "\n",
    "When the estimate is obtained say, under a square loss or a normal\n",
    "assumption, the corresponding likelihood can be written down easily. In\n",
    "such a case, likelihood ratio test under a Chi--squared distribution is\n",
    "\n",
    "$$2 (\\mathcal{L}(\\hat{\\boldsymbol{\\theta}}) - \\mathcal{L}(\\boldsymbol{\\theta})) \\le \\chi_{1 - \\alpha}^{2}(k)$$\n",
    "\n",
    "where $1-\\alpha$ is the size of the confidence region and $k$ is the\n",
    "degree of freedom. The corresponding asymptotic confidence interval for\n",
    "parameter $j$ can be derived as\n",
    "\n",
    "$$\\hat{\\theta}_{j} \\pm \\sqrt{\\chi_{1 - \\alpha}^{2}(k) H_{i,i}}.$$\n",
    "\n",
    "A pointwise confidence interval is obtained when $k = 1$. We assume in\n",
    "our package that a pointwise confidence interval is desired. This can be\n",
    "obtained simply by\n",
    "\n",
    "In \\[1\\]: from pygom import confidence_interval as ci\n",
    "\n",
    "In \\[2\\]: alpha = 0.05\n",
    "\n",
    "In \\[3\\]: xL, xU = ci.asymptotic(objSIR, alpha, xhat,\n",
    "lb=boxBoundsArray\\[:,0\\], ub=boxBoundsArray\\[:,1\\])\n",
    "\n",
    "In \\[4\\]: print(xL)\n",
    "\n",
    "In \\[5\\]: print(xU)\n",
    "\n",
    "Note that the set of bounds here is only used for check the validity of\n",
    "$\\hat{\\mathbf{x}}$ and not used in the calculation of the confidence\n",
    "intervals. Therefore the resulting output can be outside of the box\n",
    "constraints.\n",
    "\n",
    "## Profile Likelihood\n",
    "\n",
    "Another approach to calculate the confidence interval is to tackle one\n",
    "parameter at a time, treating the rest of them as nuisance parameters,\n",
    "hence the term *profile*. Let $\\mathcal{L}(\\boldsymbol{\\theta})$ be our\n",
    "log--likelihood with parameter $\\boldsymbol{\\theta}$. Element\n",
    "$\\theta_{j}$ is our parameter of interest and $\\boldsymbol{\\theta}_{-j}$\n",
    "represents the complement such that\n",
    "$\\boldsymbol{\\theta} = \\theta_{j} \\cup \\boldsymbol{\\theta}_{-j}$. For\n",
    "simply models such as linear regression with only regression\n",
    "coefficients $\\boldsymbol{\\beta}$, then\n",
    "$\\boldsymbol{\\theta} = \\boldsymbol{\\beta}$.\n",
    "\n",
    "To shorten the notation, let\n",
    "\n",
    "$$\\mathcal{L}(\\boldsymbol{\\theta}_{-j} \\mid \\theta_{j}) = \\max \\mathcal{L}(\\boldsymbol{\\theta}_{-j} \\mid \\theta_{j})$$\n",
    "\n",
    "which is the maxima of $\\boldsymbol{\\theta}_{-j}$ given $\\theta_{j}$.\n",
    "$\\hat{\\boldsymbol{\\theta}}$ denotes the MLE of the parameters as usual.\n",
    "The profile--likelihood based confidence interval for $\\theta_{j}$ is\n",
    "defined as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\theta_{j}^{U} &= \\sup \\left\\{ \\mathcal{L}(\\hat{\\boldsymbol{\\theta}}) - \\mathcal{L}(\\boldsymbol{\\theta} \\mid \\theta_{j}) \\le \\frac{1}{2} \\chi_{1 - \\alpha}^{2}(1) \\right\\} \\\\\n",
    "\\theta_{j}^{L} &= \\inf \\left\\{ \\mathcal{L}(\\hat{\\boldsymbol{\\theta}}) - \\mathcal{L}(\\boldsymbol{\\theta} \\mid \\theta_{j}) \\le \\frac{1}{2} \\chi_{1 - \\alpha}^{2}(1) \\right\\}\n",
    "\\end{aligned}$$\n",
    "\n",
    "where again we have made use of the normal approximation, but without\n",
    "imposing symmetry. The set of equations above automatically implies that\n",
    "the interval width is $\\theta_{j}^{U} - \\theta_{j}^{L}$ and\n",
    "\n",
    "$$\\mathcal{L}(\\hat{\\boldsymbol{\\theta}}) - \\frac{1}{2} \\chi_{1-\\alpha}^{2}(1) - \\mathcal{L}(\\boldsymbol{\\theta} \\mid \\theta_{j}) = 0.$$\n",
    "\n",
    "As mentioned previously, $\\boldsymbol{\\theta}_{-j}$ is the maximizer of\n",
    "the nuisance parameters, which has a gradient of zero. Combining this\n",
    "with the equation above yields a non--linear system of equations of size\n",
    "$p$,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "g(\\boldsymbol{\\theta}) = \\left[ \\begin{array}{c} \\mathcal{L}(\\boldsymbol{\\theta} \\mid \\theta_{j}) - c \\\\ \\frac{\\partial \\mathcal{L}(\\boldsymbol{\\theta} \\mid \\theta_{j})}{\\partial \\boldsymbol{\\theta}_{-j}} \\end{array} \\right] = 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "where\n",
    "$c = \\mathcal{L}(\\hat{\\boldsymbol{\\theta}}) + \\frac{1}{2} \\chi_{1-\\alpha}^{2}(1)$.\n",
    "Solving this set of system of equations only need simple Newton like\n",
    "steps, possibly with correction terms as per [\\[Venzon1988\\]](). We\n",
    "provide a function to obtain such estimate\n",
    "\n",
    "In \\[1\\]: xLProfile, xUProfile, xLProfileList, xUProfileList =\n",
    "ci.profile(objSIR, alpha, xhat, lb=boxBoundsArray\\[:,0\\],\n",
    "ub=boxBoundsArray\\[:,1\\], full_output=True)\n",
    "\n",
    "but unfortunately this is not accurate most of the time due to the\n",
    "complicated surface at locations not around $\\hat{\\theta}$. This is a\n",
    "common scenario for non--linear least square problems because the\n",
    "Hessian is not guaranteed to be a PSD everywhere. Therefore, a safeguard\n",
    "is in place to obtain the $\\theta_{j}^{U},\\theta_{j}^{L}$ by iteratively\n",
    "by updating $\\theta_{j}$ and find the solution to `nuisanceOptim`.\n",
    "\n",
    "Furthermore, we also provide the functions necessary to obtain the\n",
    "estimates such as the four below.\n",
    "\n",
    "In \\[1\\]: i = 0\n",
    "\n",
    "In \\[1\\]: funcF = ci.\\_profileF(xhat, i, 0.05, objSIR)\n",
    "\n",
    "In \\[2\\]: funcG = ci.\\_profileG(xhat, i, 0.05, objSIR)\n",
    "\n",
    "In \\[3\\]: funcGC = ci.\\_profileGSecondOrderCorrection(xhat, i, alpha,\n",
    "objSIR)\n",
    "\n",
    "In \\[4\\]: funcH = ci.\\_profileH(xhat, i, 0.05, objSIR)\n",
    "\n",
    "Where $i$ is the index of the parameter of interest. `_profileF` is the\n",
    "squared norm of `obj`, which easy the optimization process for solvers\n",
    "which requires a converted form from system of equations to non-linear\n",
    "least squares. `_profileG` is the systems of equations `obj`,\n",
    "`_profileH` is the derivative of `obj`\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla g(\\boldsymbol{\\theta}) = \\left[ \\begin{array}{c} \\frac{\\partial \\mathcal{L}(\\boldsymbol{\\theta} \\mid \\theta_{j})}{\\partial \\theta_{j}} \\\\ \\frac{\\partial^{2} \\mathcal{L}(\\boldsymbol{\\theta} \\mid \\theta_{j})}{\\partial \\boldsymbol{\\beta}_{-j} \\partial \\theta_{j}} \\end{array} \\right]\n",
    "\\end{aligned}$$\n",
    "\n",
    "and `_profileGSecondOrderCorrection` has the second order correction\n",
    "[\\[Venzon1988\\]]().\n",
    "\n",
    "## Geometric profile likelihood\n",
    "\n",
    "Due to the difficulty in obtain a profile likelihood via the standard\n",
    "Newton like steps, we also provide a way to generate a similar result\n",
    "using the geometric structure of the likelihood surface. We follow the\n",
    "method in [\\[Moolgavkar1987\\]](), which involves solving a set of\n",
    "differential equations\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{d\\beta_{j}}{dt} &= k g^{-1/2} \\\\\n",
    "\\frac{d\\boldsymbol{\\beta}_{-j}}{dt} &= \\frac{d\\boldsymbol{\\beta}_{-j}}{d\\beta_{j}} \\frac{d\\beta_{j}}{dt},\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $k = \\Phi(1-\\alpha)$ is the quantile we want to obtain under a\n",
    "normal distribution, and\n",
    "\n",
    "$$\\begin{aligned}\n",
    "g = J_{\\beta_{j}}^{\\top} I^{\\boldsymbol{\\beta}} J_{\\beta_{j}}, \\quad J_{\\beta_{j}} = \\left( \\begin{array}{c} 1 \\\\ \\frac{d\\boldsymbol{\\beta}_{-j}}{d\\beta_{j}} \\end{array} \\right).\n",
    "\\end{aligned}$$\n",
    "\n",
    "Here, $J_{\\beta_{j}}$ is the Jacobian between $\\beta_{j}$ and\n",
    "$\\boldsymbol{\\beta}_{-j}$ with the term\n",
    "\n",
    "$$\\frac{d\\boldsymbol{\\beta}_{-j}}{d\\beta_{j}} = -\\left( \\frac{\\partial^{2} \\mathcal{L}}{\\partial \\boldsymbol{\\beta}_{-j}\\partial \\boldsymbol{\\beta}_{-j}^{\\top} } \\right)^{-1} \\frac{\\partial^{2} \\mathcal{L}}{\\partial \\beta_{j} \\partial \\beta_{-j}^{\\top}}$$\n",
    "\n",
    "and hence the first element is $1$ (identity transformation).\n",
    "$I^{\\boldsymbol{\\beta}}$ is the Fisher information of\n",
    "$\\boldsymbol{\\beta}$, which is\n",
    "\n",
    "$$I^{\\boldsymbol{\\beta}} = \\frac{\\partial \\boldsymbol{\\theta}}{\\partial \\boldsymbol{\\beta}^{\\top}} \\Sigma^{\\boldsymbol{\\theta}(\\boldsymbol{\\beta})} \\frac{\\partial \\boldsymbol{\\theta}}{\\partial \\boldsymbol{\\beta}}.$$\n",
    "\n",
    "It is simply $\\Sigma^{\\boldsymbol{\\beta}}$ if\n",
    "$\\boldsymbol{\\theta} = \\boldsymbol{\\beta}$. Different Fisher information\n",
    "can be used for $\\Sigma^{\\boldsymbol{\\beta}}$ such as the expected or\n",
    "observed, at $\\hat{\\boldsymbol{\\beta}}$ or $\\boldsymbol{\\beta}$. After\n",
    "some trivial algebraic manipulation, we can show that our ode boils\n",
    "downs to\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\left[ \\begin{array}{c} \\frac{d\\beta_{j}}{dt} \\\\ \\frac{d\\boldsymbol{\\beta_{-j}}}{dt} \\end{array} \\right] = k \\left[ \\begin{array}{c} 1 \\\\ -A^{-1}w \\end{array} \\right] \\left( v - w^{\\top}A^{-1}w \\right)^{-1/2}\n",
    "\\end{aligned}$$\n",
    "\n",
    "where the symbols on the RHS above correspond to partitions in the\n",
    "Fisher information\n",
    "\n",
    "$$\\begin{aligned}\n",
    "I^{\\boldsymbol{\\beta}} = \\left[ \\begin{array}{cc} v & w^{\\top} \\\\ w & A \\end{array} \\right].\n",
    "\\end{aligned}$$\n",
    "\n",
    "The integration is perform from $t = 0$ to $1$ and is all handled\n",
    "internally via `geometric`\n",
    "\n",
    "In \\[1\\]: xLGeometric, xUGeometric, xLList, xUList =\n",
    "ci.geometric(objSIR, alpha, xhat, full_output=True)\n",
    "\n",
    "In \\[2\\]: print(xLGeometric)\n",
    "\n",
    "In \\[3\\]: print(xUGeometric)\n",
    "\n",
    "## Bootstrap\n",
    "\n",
    "This is perhaps the favorite method to estimate confidence interval for\n",
    "a lot of people. Although there are many ways to implement bootstrap,\n",
    "semi-parametric is the only logical choice (even though the underlying\n",
    "assumptions may be violated at times). As we have only implemented OLS\n",
    "type loss functions in this package, the parametric approach seem to be\n",
    "inappropriate when there is no self--efficiency guarantee.\n",
    "Non-parametric approach requires at least a conditional independence\n",
    "assumption, something easily violated by our **ode**. Block bootstrap is\n",
    "an option but we are also aware that the errors of an **ode** can be\n",
    "rather rigid, and consistently over/under estimate at certain periods of\n",
    "time.\n",
    "\n",
    "When we say semi-parametric, we mean the exchange of errors between the\n",
    "observations. Let our raw error be\n",
    "\n",
    "$$\\varepsilon_{i} = y_{i} - \\hat{y}_{i}$$\n",
    "\n",
    "where $\\hat{y}_{i}$ will be the prediction under\n",
    "$\\hat{\\boldsymbol{\\theta}}$ under our model. Then we construct a new set\n",
    "of observations via\n",
    "\n",
    "$$y_{i}^{\\ast} = \\hat{y}_{i} + \\varepsilon^{\\ast}, \\quad \\varepsilon^{\\ast} \\sim \\mathcal{F}$$\n",
    "\n",
    "with $\\mathcal{F}$ being the empirical distribution of the raw errors. A\n",
    "new set of parameters $\\theta^{\\ast}$ are then found for the\n",
    "bootstrapped samples, and we obtain the $\\alpha$ confidence interval by\n",
    "taking the $\\alpha/2$ quantiles. Invoke the correspond python function\n",
    "yields our bootstrap estimates. Unlike `asymptotic`, the bounds here are\n",
    "used when estimating the parameters of each bootstrap samples. An error\n",
    "may be returned if estimation failed for any of the bootstrap samples.\n",
    "\n",
    "In \\[1\\]: xLBootstrap, xUBootstrap, setX = ci.bootstrap(objSIR, alpha,\n",
    "xhat, iteration=10, lb=boxBoundsArray\\[:,0\\], ub=boxBoundsArray\\[:,1\\],\n",
    "full_output=True)\n",
    "\n",
    "In \\[2\\]: print(xLBootstrap)\n",
    "\n",
    "In \\[3\\]: print(xUBootstrap)\n",
    "\n",
    "The additional information here can be used to compute the bias, tail\n",
    "effects and test against the normality assumption. If desired, a\n",
    "simultaneous confidence interval can also be approximated empirically.\n",
    "Note however that because we are using a semi--parameter method here, if\n",
    "the model specification is wrong then the resulting estimates for the\n",
    "bias is also wrong. The confidence interval still has the normal\n",
    "approximation guarantee if number of sample is large.\n",
    "\n",
    "In this case, because the error in the observation is extremely small,\n",
    "the confidence interval is narrow.\n",
    "\n",
    "In \\[1\\]: import pylab as P\n",
    "\n",
    "In \\[2\\]: f = plt.figure()\n",
    "\n",
    "In \\[3\\]: n, bins, patches = P.hist(setX\\[:,0\\], 50)\n",
    "\n",
    "In \\[4\\]: P.xlabel(r'Estimates of \\$beta\\$');\n",
    "\n",
    "In \\[5\\]: P.ylabel('Frequency');\n",
    "\n",
    "In \\[6\\]: P.title('Estimates under a semi-parametric bootstrap scheme');\n",
    "\n",
    "@savefig bootstrapCIHist.png In \\[7\\]: P.show()\n",
    "\n",
    "In \\[8\\]: P.close()\n",
    "\n",
    "## Comparison Between Methods\n",
    "\n",
    "Although we have shown the numerical values for the confidence interval\n",
    "obtained using different method, it may be hard to comprehend how they\n",
    "vary. As they say, a picture says a million word, and given that this\n",
    "particular model only has two parameters, we can obtain inspect and\n",
    "compare the methods visually via a contour plot. The code to perform\n",
    "this is shown below but the code block will not be run to save time and\n",
    "space.\n",
    "\n",
    "In the plot above, the bootstrap confidence interval were so close to\n",
    "the MLE, it is impossible to distinguish the two on such a coarse scale.\n",
    "\n",
    "Furthermore, because the geometric confidence interval is the result of\n",
    "an integration, we can trace the path that lead to the final output that\n",
    "was shown previously. Again, we are space conscious (and time\n",
    "constrained) so the code block below will not be run.\n",
    "\n",
    "In \\[1\\]: fig = plt.figure()\n",
    "\n",
    "In \\[2\\]: CS = plt.contour(xi, yi, zi, linewidth=0.5)\n",
    "\n",
    "In \\[3\\]: plt.clabel(CS, fontsize=10, inline=1)\n",
    "\n",
    "In \\[4\\]: l1 = plt.scatter(xLList\\[0\\]\\[:,0\\], xLList\\[0\\]\\[:,1\\],\n",
    "marker='o', c='m', s=10);\n",
    "\n",
    "In \\[5\\]: l2 = plt.scatter(xUList\\[0\\]\\[:,0\\], xUList\\[0\\]\\[:,1\\],\n",
    "marker='x', c='m', s=10);\n",
    "\n",
    "In \\[6\\]: plt.legend((l1, l2), ('Lower CI path', 'Upper CI path'),\n",
    "loc='upper left');\n",
    "\n",
    "In \\[7\\]: plt.ylabel(r'Estimates of \\$gamma\\$');\n",
    "\n",
    "In \\[8\\]: plt.xlabel(r'Estimates of \\$beta\\$');\n",
    "\n",
    "In \\[9\\]: plt.title('Integration path of the geometric confidence\n",
    "intervals on the likelihood surface');\n",
    "\n",
    "In \\[10\\]: plt.tight_layout();\n",
    "\n",
    "In \\[11\\]: plt.show()\n",
    "\n",
    "In \\[12\\]: plt.close()\n",
    "\n",
    "## Profile Likelihood Surface\n",
    "\n",
    "To investigate why it was hard to find the profile likelihood confidence\n",
    "interval, we can simply look at the surface (which is simply a line as\n",
    "we are profiling). We find solution of `nuisanceOptim` for each\n",
    "$\\boldsymbol{\\theta}_{-j}$ at various points of $\\boldsymbol{\\theta}$.\n",
    "Equivalently, we can minimize the original loss function as defined\n",
    "previously, and this is the approach below. We focus out attention to\n",
    "the parameter $\\beta$ of our SIR model. The results are not shown here\n",
    "but the existence of a solution to `obj` is evident by simply\n",
    "*eyeballing* the plots.\n",
    "\n",
    "In \\[1\\]: numIter = 100\n",
    "\n",
    "In \\[2\\]: x2 = numpy.linspace(0.0, 2.0, numIter)\n",
    "\n",
    "In \\[3\\]: funcOut = numpy.linspace(0.0, 2.0, numIter)\n",
    "\n",
    "In \\[4\\]: ode.parameters = \\[('beta',0.5), ('gamma',1.0/3.0)\\]\n",
    "\n",
    "In \\[5\\]: for i in range(numIter):  \n",
    "...: paramEval = \\[('beta',x2\\[i\\]), ('gamma',x2\\[i\\])\\] ...: ode2 =\n",
    "copy.deepcopy(ode) ...: ode2.parameters = paramEval ...:\n",
    "ode2.initial_values = (x0, t\\[0\\]) ...: objSIR2 = NormalLoss(x2\\[i\\],\n",
    "ode2, x0, t\\[0\\], t\\[1::\\], yObv.copy(), targetState,\n",
    "target_param='gamma') ...: res =\n",
    "scipy.optimize.minimize(fun=objSIR2.cost, ...: jac=objSIR2.gradient,\n",
    "...: x0=x2\\[i\\], ...: bounds=\\[(0,2)\\], ...: method='L-BFGS-B') ...:\n",
    "funcOut\\[i\\] = res\\['fun'\\]\n",
    "\n",
    "In \\[10\\]: fig = plt.figure()\n",
    "\n",
    "In \\[10\\]: plt.plot(x2, objSIR.cost(xhat) - funcOut)\n",
    "\n",
    "In \\[11\\]: l1 = plt.axhline(-0.5\\*qchisq(1 - alpha, df=1), 0, 2,\n",
    "color='r')\n",
    "\n",
    "In \\[12\\]: plt.ylabel(r'\\$mathcal{L}(hat{theta}) - mathcal{L}(theta mid\n",
    "beta)\\$');\n",
    "\n",
    "In \\[13\\]: plt.xlabel(r'Fixed value of \\$beta\\$');\n",
    "\n",
    "In \\[14\\]: plt.title('Difference in objective function between MLEn and\n",
    "the maximization of the nuisance parameters given then parameter of\n",
    "interest, beta in this case');\n",
    "\n",
    "In \\[15\\]: plt.tight_layout();\n",
    "\n",
    "In \\[16\\]: plt.legend((l1,), (r'\\$-0.5mathcal{X}\\_{1 -\n",
    "alpha}^{2}(1)\\$',), loc='lower right');\n",
    "\n",
    "@savefig profileLLMaximizerGivenBeta.png In \\[17\\]: plt.show() \\#\n",
    "@savefig profileLLMaximizerGivenBeta.png\n",
    "\n",
    "In \\[18\\]: plt.close()\n",
    "\n",
    "Both the upper and lower confidence interval can be found in the\n",
    "profiling procedure, but the part between of\n",
    "$\\beta \\in \\left[0,\\hat{\\beta}\\right]$ is not convex, with $\\hat{\\beta}$\n",
    "being the MLE. This non--quadratic profile likelihood is due to the\n",
    "non-identifiability of the model given data [\\[Raue2009\\]](). For this\n",
    "particular case, we can fix it simply by introducing additional\n",
    "observations in the form of the $I$ state. We encourage the users to try\n",
    "it out for themselves to confirm.\n",
    "\n",
    "In \\[1\\]: targetState = \\['I', 'R'\\]\n",
    "\n",
    "In \\[2\\]: targetStateIndex =\n",
    "numpy.array(ode.get_state_index(targetState))\n",
    "\n",
    "In \\[3\\]: y = solution\\[1::,targetStateIndex\\] + numpy.random.normal(0,\n",
    "0.01, (len(solution\\[1::,targetStateIndex\\]), 1))\n",
    "\n",
    "In \\[4\\]: objSIR = NormalLoss(theta, ode, x0, t\\[0\\], t\\[1::\\],\n",
    "y.copy(), targetState)\n",
    "\n",
    "In \\[5\\]: xhat = objSIR.fit(theta, lb=boxBoundsArray\\[:,0\\],\n",
    "ub=boxBoundsArray\\[:,1\\])\n",
    "\n",
    "In \\[6\\]: for i in range(numIter):  \n",
    "...: paramEval = \\[('beta', x2\\[i\\]), ('gamma', x2\\[i\\])\\] ...: ode2 =\n",
    "copy.deepcopy(ode) ...: ode2.parameters = paramEval ...:\n",
    "ode2.initial_values = (x0, t\\[0\\]) ...: objSIR2 = NormalLoss(x2\\[i\\],\n",
    "ode2, x0, t\\[0\\], t\\[1::\\], y.copy(), targetState, target_param='gamma')\n",
    "...: res = scipy.optimize.minimize(fun=objSIR2.cost, ...:\n",
    "jac=objSIR2.gradient, ...: x0=x2\\[i\\], ...: bounds=\\[(0,2)\\], ...:\n",
    "method='L-BFGS-B') ...: funcOut\\[i\\] = res\\['fun'\\]\n",
    "\n",
    "In \\[10\\]: fig = plt.figure()\n",
    "\n",
    "In \\[10\\]: plt.plot(x2, objSIR.cost(xhat) - funcOut);\n",
    "\n",
    "In \\[11\\]: l1 = plt.axhline(-0.5\\*qchisq(1 - alpha, df=1), 0, 2,\n",
    "color='r')\n",
    "\n",
    "In \\[12\\]: plt.ylabel(r'\\$mathcal{L}(hat{theta}) - mathcal{L}(theta mid\n",
    "beta)\\$');\n",
    "\n",
    "In \\[13\\]: plt.xlabel(r'Fixed value of \\$beta\\$');\n",
    "\n",
    "In \\[14\\]: plt.title('Profile likelihood curve for the parameter ofn\n",
    "interest with more observation');\n",
    "\n",
    "In \\[15\\]: plt.tight_layout();\n",
    "\n",
    "In \\[16\\]: plt.legend((l1,), (r'\\$-0.5mathcal{X}\\_{1 -\n",
    "alpha}^{2}(1)\\$',), loc='lower right');\n",
    "\n",
    "@savefig profileLLMaximizerGivenBetaMoreObs.png In \\[17\\]: plt.show() \\#\n",
    "@savefig profileLLMaximizerGivenBetaMoreObs.png\n",
    "\n",
    "In \\[18\\]: plt.close()"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
